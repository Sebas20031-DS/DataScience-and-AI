{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Python version: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
      "Using TensorFlow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.activations import relu, softmax\n",
    "\n",
    "# Used to prepare the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set random seeds\n",
    "# for consistent results, \n",
    "# If you remove the next lines, the results will not be reproducible \n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Ensure deterministic operations (only for GPU execution)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# Python version in the environment (to use tensorflow it should be 3.11.* or lower)\n",
    "# I used 3.11.9\n",
    "import sys\n",
    "print(f\"Using Python version: {sys.version}\")\n",
    "\n",
    "# TensorFlow version\n",
    "print(f\"Using TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Loading and splitting the data* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (70000, 28, 28)\n",
      "y shape: (70000,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Combine training and test data for reshuffling\n",
    "x_original = np.concatenate((x_train, x_test), axis=0)\n",
    "y_original = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "print(\"x shape:\", x_original.shape)\n",
    "print(\"y shape:\", y_original.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Devide the data into 3 sets (training, cross validation and test) to compare each model performance later*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training set (input) is: (49000, 28, 28)\n",
      "the shape of the training set (target) is: (49000,)\n",
      "\n",
      "the shape of the cross validation set (input) is: (14070, 28, 28)\n",
      "the shape of the cross validation set (target) is: (14070,)\n",
      "\n",
      "the shape of the test set (input) is: (6930, 28, 28)\n",
      "the shape of the test set (target) is: (6930,)\n"
     ]
    }
   ],
   "source": [
    "# Get 60% of the dataset as the training set. \n",
    "# Put the remaining 40% in temporary variables: x_ and y_.\n",
    "\n",
    "x_train, x_, y_train, y_ = train_test_split(x_original, y_original, test_size = 0.3, random_state=1)\n",
    "\n",
    "# Split the 40% subset above into two: \n",
    "# one half for cross validation and the other for the test set\n",
    "\n",
    "x_dev, x_test, y_dev, y_test = train_test_split(x_, y_, test_size = 0.33, random_state=1)\n",
    "\n",
    "del x_, y_\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_dev.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_dev.shape}\\n\")\n",
    "print(f\"the shape of the test set (input) is: {x_test.shape}\")\n",
    "print(f\"the shape of the test set (target) is: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Normalize the data to improve performance and avoid biased weights*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "x_train = x_train / 255.0\n",
    "x_dev = x_dev / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Ensure CNN compatibility\n",
    "x_train_cnn = x_train.reshape(-1, 28, 28, 1)\n",
    "x_dev_cnn = x_dev.reshape(-1, 28, 28, 1)\n",
    "x_test_cnn = x_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Models implementation and training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS IMPLEMENTATION\n",
    "\n",
    "models = [\n",
    "    Sequential([Flatten(input_shape=(28, 28)), Dense(10, activation='softmax')], name='Baseline_Model'),\n",
    "    Sequential([Flatten(input_shape=(28, 28)), Dense(64, activation='relu'), Dense(10, activation='softmax')], name='Small_Model'),\n",
    "    Sequential([Flatten(input_shape=(28, 28)), Dense(32, activation='relu'), Dense(16, activation='relu'), Dense(10, activation='softmax')], name='Medium_Model'),\n",
    "    Sequential([\n",
    "        Conv2D(16, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "        Flatten(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ], name='CNN_Model')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Baseline_Model...\n",
      "Done!\n",
      "\n",
      "Training Small_Model...\n",
      "Done!\n",
      "\n",
      "Training Medium_Model...\n",
      "Done!\n",
      "\n",
      "Training CNN_Model...\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compiling all models\n",
    "for model in models:\n",
    "\n",
    "    # Setup the loss and optimizer\n",
    "    model.compile(\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "    )\n",
    "\n",
    "    print(f\"Training {model.name}...\")\n",
    "\n",
    "    # Train the model\n",
    "    x_train_input = x_train_cnn if 'CNN' in model.name else x_train\n",
    "    model.fit(\n",
    "        x_train_input, y_train,\n",
    "        epochs = 100,\n",
    "        verbose = 0   # Silent mode, no output during training.\n",
    "    )\n",
    "    \n",
    "    print(\"Done!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Evaluating the models* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First compare the training error with the cross validation error and choose the one with the smallest cross validation error but also  avoiding overfitting in case the training error is to small comparing with the cross validation error*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1532/1532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m1532/1532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m1532/1532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m1532/1532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "RESULTS:\n",
      "Model 1: Training Error: 5.62%, Cross validation Error: 8.22%\n",
      "Model 2: Training Error: 0.02%, Cross validation Error: 2.73%\n",
      "Model 3: Training Error: 0.69%, Cross validation Error: 4.02%\n",
      "Model 4: Training Error: 0.00%, Cross validation Error: 1.53%\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists that will contain the errors for each model\n",
    "nn_train_errors = []\n",
    "nn_cv_errors = []\n",
    "\n",
    "# Calculate the errors\n",
    "for model in models:\n",
    "    x_train_input = x_train_cnn if 'CNN' in model.name else x_train\n",
    "    x_dev_input = x_dev_cnn if 'CNN' in model.name else x_dev\n",
    "\n",
    "    # Calculate the training error and store it\n",
    "    train_predictions = model.predict(x_train_input)\n",
    "    train_accuracy = accuracy_score(y_train, train_predictions.argmax(axis=1))\n",
    "    nn_train_errors.append(1 - train_accuracy)\n",
    "\n",
    "    # Calculate the cross-validation error and store it\n",
    "    dev_predictions = model.predict(x_dev_input)\n",
    "    dev_accuracy = accuracy_score(y_dev, dev_predictions.argmax(axis=1))\n",
    "    nn_cv_errors.append(1 - dev_accuracy)\n",
    "\n",
    "# Comparing the errors for all models\n",
    "print(\"\\nRESULTS:\")\n",
    "\n",
    "for idx, (train_error, dev_error) in enumerate(zip(nn_train_errors, nn_cv_errors), 1):\n",
    "    print(f\"Model {idx}: Training Error: {train_error:.2%}, Cross validation Error: {dev_error:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Generally perfect training error indicates an issue of overfitting.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Chosing a model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For illustrative purposes in this notebook we test all the models with the test set, but in practice this can be done only with the chossed model based on performance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "fraction of misclassified data for Baseline_Model: 0.08282828282828283\n",
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "fraction of misclassified data for Small_Model: 0.02886002886002886\n",
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "fraction of misclassified data for Medium_Model: 0.04040404040404041\n",
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "fraction of misclassified data for CNN_Model: 0.016305916305916306\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "\n",
    "    x_test_input = x_test_cnn if 'CNN' in model.name else x_test\n",
    "\n",
    "    # Predict class probabilities for the training set using models\n",
    "    predictions = model.predict(x_test_input)\n",
    "\n",
    "    # Get the predicted class labels\n",
    "    yhat = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Misclassified data\n",
    "    misclassified = sum(yhat != y_test)\n",
    "\n",
    "    # Compute the fraction of the data that the model misclassified\n",
    "    fraction_error = misclassified / len(predictions)\n",
    "    \n",
    "    print(f\"fraction of misclassified data for {model.name}: {fraction_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can now save the model with the best performance on the test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the best model based on the performance over the test set\n",
    "# uncomment the line below to save the model\n",
    "\n",
    "models[1].save('M1_digit_rec.keras')\n",
    "models[3].save('M2_digit_rec.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
